{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf443f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e03e495",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=requests.get(\"http://www.gutenberg.org/cache/epub/5200/pg5200.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe457b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg EBook of Metamorphosis, by Franz Kafka\\r\\nTranslated by David Wyllie.\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and with\\r\\nalmost no restrictions whatsoever.  You may copy it, give it away or\\r\\nre-use it under the terms of the Project Gutenberg License included\\r\\nwith this eBook or online at www.gutenberg.org\\r\\n\\r\\n** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **\\r\\n**     Please follow the copyright guidelines in this file.     **\\r\\n\\r\\n\\r\\nTitle: Metamorphosis\\r\\n\\r\\nAuthor: Franz Kafka\\r\\n\\r\\nTranslator: David Wyllie\\r\\n\\r\\nRelease Date: August 16, 2005 [EBook #5200]\\r\\nFirst posted: May 13, 2002\\r\\nLast updated: May 20, 2012\\r\\n\\r\\nLanguage: English\\r\\n\\r\\n\\r\\n*** START OF THIS PROJECT GUTENBERG EBOOK METAMORPHOSIS ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nCopyright (C) 2002 David Wyllie.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n  Metamorphosis\\r\\n  Franz Kafka\\r\\n\\r\\nTranslated by David Wyllie\\r\\n\\r\\n\\r\\n\\r\\nI\\r\\n\\r\\n\\r\\nOne morning, when Gregor Samsa woke from troubled dreams, he found\\r\\nhimself transformed in his bed into a horrible vermin.  He lay on\\r\\nhis armour-like back, and if he lifted his head a little he could\\r\\nsee his brown belly, slightly domed and divided by arches into stiff\\r\\nsections.  The bedding was hardly able to cover it and seemed ready\\r\\nto slide off any moment.  His many legs, pitifully thin compared\\r\\nwith the size of the rest of him, waved about helplessly as he\\r\\nlooked.\\r\\n\\r\\n\"What\\'s happened to me?\" he thought.  It wasn\\'t a dream.  His room,\\r\\na proper human room although a little too small, lay peacefully\\r\\nbetwee'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f442d79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg EBook of Metamorphosis, by Franz Kafka\\r'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = response.text.split('\\n')\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d8d1707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'away from the bed, bend down with the load and then be patient and\\r'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data[253:]\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5870b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2110"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11d29a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'away from the bed, bend down with the load and then be patient and\\r careful as he swang over onto the floor, where, hopefully, the\\r little legs would find a use.  Should he really call for help\\r though, even apart from the fact that all the doors were locked?\\r Despite all the difficulty he was in, he could not suppress a smile\\r at this thought.\\r \\r After a while he had already moved so far across that it would have\\r been hard for him to keep his balance if he rocked too hard.  The\\r time was now ten past seven and he would have to make a final\\r decision very soon.  Then there was a ring at the door of the flat.\\r \"That\\'ll be someone from work\", he said to himself, and froze very\\r still, although his little legs only became all the more lively as\\r they danced around.  For a moment everything remained quiet.\\r \"They\\'re not opening the door\", Gregor said to himself, caught in\\r some nonsensical hope.  But then of course, the maid\\'s firm steps\\r went to the door as ever and opened it.  Gregor on'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \" \".join(data)\n",
    "data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee33c072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['away', 'from', 'the', 'bed', 'bend', 'down', 'with', 'the', 'load', 'and', 'then', 'be', 'patient', 'and', 'careful', 'as', 'he', 'swang', 'over', 'onto', 'the', 'floor', 'where', 'hopefully', 'the', 'little', 'legs', 'would', 'find', 'a', 'use', 'should', 'he', 'really', 'call', 'for', 'help', 'though', 'even', 'apart', 'from', 'the', 'fact', 'that', 'all', 'the', 'doors', 'were', 'locked', 'despite']\n"
     ]
    }
   ],
   "source": [
    "def clean_text(doc):\n",
    " tokens = doc.split()\n",
    " table = str.maketrans('', '', string.punctuation)\n",
    " tokens = [w.translate(table) for w in tokens]\n",
    " tokens = [word for word in tokens if word.isalpha()]\n",
    " tokens = [word.lower() for word in tokens]\n",
    " return tokens\n",
    "tokens = clean_text(data)\n",
    "print(tokens[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "169d26ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22607"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc959d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22556\n"
     ]
    }
   ],
   "source": [
    "length = 50 + 1\n",
    "lines = []\n",
    "for i in range(length, len(tokens)):\n",
    " seq = tokens[i-length:i]\n",
    " line = ' '.join(seq)\n",
    " lines.append(line)\n",
    " if i > 200000:\n",
    "     break\n",
    "print(len(lines))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f76cde",
   "metadata": {},
   "source": [
    "##### Build LSTM Model and Prepare X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fcfec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18f2a092",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8356a612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 103,   29,    1,  245, 2883,   98,   14,    1, 1435,    3,   48,\n",
       "         30,  618,    3,  756,   13,    6, 1434,  107,  165,    1,  149,\n",
       "         86, 2880,    1,   78,  225,   21,  530,   12,  156,  193,    6,\n",
       "        142,  754,   17,  180,  116,   49, 1433,   29,    1,  753,   11,\n",
       "         26,    1,  455,   58,  617,  329])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:, :-1], sequences[:,-1]\n",
    "X[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65c96619",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99646860",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e4f76c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = X.shape[1]\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1e2996",
   "metadata": {},
   "source": [
    "##### LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d085b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2023d217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 50)            144250    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2885)              291385    \n",
      "=================================================================\n",
      "Total params: 586,535\n",
      "Trainable params: 586,535\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7036fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2e86362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "89/89 [==============================] - 38s 370ms/step - loss: 6.6375 - accuracy: 0.0441\n",
      "Epoch 2/100\n",
      "89/89 [==============================] - 32s 364ms/step - loss: 6.1886 - accuracy: 0.0540\n",
      "Epoch 3/100\n",
      "89/89 [==============================] - 32s 364ms/step - loss: 6.1634 - accuracy: 0.0540\n",
      "Epoch 4/100\n",
      "89/89 [==============================] - 33s 373ms/step - loss: 6.0537 - accuracy: 0.0540\n",
      "Epoch 5/100\n",
      "89/89 [==============================] - 32s 358ms/step - loss: 5.9814 - accuracy: 0.0540\n",
      "Epoch 6/100\n",
      "89/89 [==============================] - 33s 366ms/step - loss: 5.8502 - accuracy: 0.0587\n",
      "Epoch 7/100\n",
      "89/89 [==============================] - 33s 365ms/step - loss: 5.7178 - accuracy: 0.0694\n",
      "Epoch 8/100\n",
      "89/89 [==============================] - 31s 352ms/step - loss: 5.6116 - accuracy: 0.0742\n",
      "Epoch 9/100\n",
      "89/89 [==============================] - 31s 351ms/step - loss: 5.5323 - accuracy: 0.0793\n",
      "Epoch 10/100\n",
      "89/89 [==============================] - 31s 347ms/step - loss: 5.4658 - accuracy: 0.0845\n",
      "Epoch 11/100\n",
      "89/89 [==============================] - 31s 346ms/step - loss: 5.4088 - accuracy: 0.0890\n",
      "Epoch 12/100\n",
      "89/89 [==============================] - 31s 353ms/step - loss: 5.3401 - accuracy: 0.0939\n",
      "Epoch 13/100\n",
      "89/89 [==============================] - 31s 343ms/step - loss: 5.2654 - accuracy: 0.1005\n",
      "Epoch 14/100\n",
      "89/89 [==============================] - 31s 344ms/step - loss: 5.1927 - accuracy: 0.1047\n",
      "Epoch 15/100\n",
      "89/89 [==============================] - 31s 345ms/step - loss: 5.1223 - accuracy: 0.1099\n",
      "Epoch 16/100\n",
      "89/89 [==============================] - 31s 347ms/step - loss: 5.0441 - accuracy: 0.1128\n",
      "Epoch 17/100\n",
      "89/89 [==============================] - 31s 345ms/step - loss: 4.9574 - accuracy: 0.1189\n",
      "Epoch 18/100\n",
      "89/89 [==============================] - 31s 352ms/step - loss: 4.8848 - accuracy: 0.1238\n",
      "Epoch 19/100\n",
      "89/89 [==============================] - 32s 353ms/step - loss: 4.8203 - accuracy: 0.1283\n",
      "Epoch 20/100\n",
      "89/89 [==============================] - 32s 361ms/step - loss: 4.7617 - accuracy: 0.1338\n",
      "Epoch 21/100\n",
      "89/89 [==============================] - 33s 370ms/step - loss: 4.7130 - accuracy: 0.1374\n",
      "Epoch 22/100\n",
      "89/89 [==============================] - 32s 355ms/step - loss: 4.6622 - accuracy: 0.1376\n",
      "Epoch 23/100\n",
      "89/89 [==============================] - 32s 358ms/step - loss: 4.6187 - accuracy: 0.1415\n",
      "Epoch 24/100\n",
      "89/89 [==============================] - 34s 379ms/step - loss: 4.5763 - accuracy: 0.1449\n",
      "Epoch 25/100\n",
      "89/89 [==============================] - 32s 356ms/step - loss: 4.5414 - accuracy: 0.1474\n",
      "Epoch 26/100\n",
      "89/89 [==============================] - 32s 364ms/step - loss: 4.4990 - accuracy: 0.1497\n",
      "Epoch 27/100\n",
      "89/89 [==============================] - 35s 392ms/step - loss: 4.4618 - accuracy: 0.1515\n",
      "Epoch 28/100\n",
      "89/89 [==============================] - 35s 397ms/step - loss: 4.4256 - accuracy: 0.1566\n",
      "Epoch 29/100\n",
      "89/89 [==============================] - 34s 385ms/step - loss: 4.3910 - accuracy: 0.1565\n",
      "Epoch 30/100\n",
      "89/89 [==============================] - 35s 389ms/step - loss: 4.3545 - accuracy: 0.1586\n",
      "Epoch 31/100\n",
      "89/89 [==============================] - 36s 400ms/step - loss: 4.3222 - accuracy: 0.1632\n",
      "Epoch 32/100\n",
      "89/89 [==============================] - 33s 368ms/step - loss: 4.2881 - accuracy: 0.1637\n",
      "Epoch 33/100\n",
      "89/89 [==============================] - 32s 359ms/step - loss: 4.2570 - accuracy: 0.1644\n",
      "Epoch 34/100\n",
      "89/89 [==============================] - 33s 371ms/step - loss: 4.2242 - accuracy: 0.1655\n",
      "Epoch 35/100\n",
      "89/89 [==============================] - 38s 428ms/step - loss: 4.1929 - accuracy: 0.1702\n",
      "Epoch 36/100\n",
      "89/89 [==============================] - 35s 397ms/step - loss: 4.1670 - accuracy: 0.1698\n",
      "Epoch 37/100\n",
      "89/89 [==============================] - 34s 377ms/step - loss: 4.1350 - accuracy: 0.1714\n",
      "Epoch 38/100\n",
      "89/89 [==============================] - 34s 379ms/step - loss: 4.1035 - accuracy: 0.1747\n",
      "Epoch 39/100\n",
      "89/89 [==============================] - 33s 371ms/step - loss: 4.0724 - accuracy: 0.1776\n",
      "Epoch 40/100\n",
      "89/89 [==============================] - 32s 359ms/step - loss: 4.0390 - accuracy: 0.1803\n",
      "Epoch 41/100\n",
      "89/89 [==============================] - 32s 362ms/step - loss: 4.0113 - accuracy: 0.1831\n",
      "Epoch 42/100\n",
      "89/89 [==============================] - 32s 358ms/step - loss: 3.9814 - accuracy: 0.1836\n",
      "Epoch 43/100\n",
      "89/89 [==============================] - 34s 377ms/step - loss: 3.9531 - accuracy: 0.1846\n",
      "Epoch 44/100\n",
      "89/89 [==============================] - 34s 387ms/step - loss: 3.9254 - accuracy: 0.1887\n",
      "Epoch 45/100\n",
      "89/89 [==============================] - 32s 361ms/step - loss: 3.8940 - accuracy: 0.1918\n",
      "Epoch 46/100\n",
      "89/89 [==============================] - 32s 359ms/step - loss: 3.8737 - accuracy: 0.1921\n",
      "Epoch 47/100\n",
      "89/89 [==============================] - 32s 362ms/step - loss: 3.8370 - accuracy: 0.1980\n",
      "Epoch 48/100\n",
      "89/89 [==============================] - 32s 361ms/step - loss: 3.8069 - accuracy: 0.2004\n",
      "Epoch 49/100\n",
      "89/89 [==============================] - 32s 362ms/step - loss: 3.7771 - accuracy: 0.2006\n",
      "Epoch 50/100\n",
      "89/89 [==============================] - 32s 358ms/step - loss: 3.7494 - accuracy: 0.2057\n",
      "Epoch 51/100\n",
      "89/89 [==============================] - 34s 378ms/step - loss: 3.7141 - accuracy: 0.2115\n",
      "Epoch 52/100\n",
      "89/89 [==============================] - 36s 406ms/step - loss: 3.6897 - accuracy: 0.2131\n",
      "Epoch 53/100\n",
      "89/89 [==============================] - 33s 372ms/step - loss: 3.6614 - accuracy: 0.2145\n",
      "Epoch 54/100\n",
      "89/89 [==============================] - 32s 361ms/step - loss: 3.6314 - accuracy: 0.2200\n",
      "Epoch 55/100\n",
      "89/89 [==============================] - 33s 370ms/step - loss: 3.6018 - accuracy: 0.2197\n",
      "Epoch 56/100\n",
      "89/89 [==============================] - 33s 369ms/step - loss: 3.5688 - accuracy: 0.2260\n",
      "Epoch 57/100\n",
      "89/89 [==============================] - 32s 359ms/step - loss: 3.5397 - accuracy: 0.2292\n",
      "Epoch 58/100\n",
      "89/89 [==============================] - 32s 360ms/step - loss: 3.5152 - accuracy: 0.2361\n",
      "Epoch 59/100\n",
      "89/89 [==============================] - 32s 357ms/step - loss: 3.4866 - accuracy: 0.2359\n",
      "Epoch 60/100\n",
      "89/89 [==============================] - 32s 365ms/step - loss: 3.4594 - accuracy: 0.2394\n",
      "Epoch 61/100\n",
      "89/89 [==============================] - 33s 366ms/step - loss: 3.4254 - accuracy: 0.2441\n",
      "Epoch 62/100\n",
      "89/89 [==============================] - 32s 359ms/step - loss: 3.4034 - accuracy: 0.2467\n",
      "Epoch 63/100\n",
      "89/89 [==============================] - 32s 361ms/step - loss: 3.3717 - accuracy: 0.2534\n",
      "Epoch 64/100\n",
      "89/89 [==============================] - 32s 363ms/step - loss: 3.3503 - accuracy: 0.2574\n",
      "Epoch 65/100\n",
      "89/89 [==============================] - 32s 361ms/step - loss: 3.3245 - accuracy: 0.2599\n",
      "Epoch 66/100\n",
      "89/89 [==============================] - 32s 360ms/step - loss: 3.2999 - accuracy: 0.2604\n",
      "Epoch 67/100\n",
      "89/89 [==============================] - 32s 362ms/step - loss: 3.2711 - accuracy: 0.2663\n",
      "Epoch 68/100\n",
      "89/89 [==============================] - 32s 360ms/step - loss: 3.2402 - accuracy: 0.2713\n",
      "Epoch 69/100\n",
      "89/89 [==============================] - 32s 362ms/step - loss: 3.2140 - accuracy: 0.2720\n",
      "Epoch 70/100\n",
      "89/89 [==============================] - 33s 375ms/step - loss: 3.1899 - accuracy: 0.2777\n",
      "Epoch 71/100\n",
      "89/89 [==============================] - 33s 366ms/step - loss: 3.1627 - accuracy: 0.2813\n",
      "Epoch 72/100\n",
      "89/89 [==============================] - 32s 360ms/step - loss: 3.1428 - accuracy: 0.2852\n",
      "Epoch 73/100\n",
      "89/89 [==============================] - 32s 360ms/step - loss: 3.1111 - accuracy: 0.2903\n",
      "Epoch 74/100\n",
      "89/89 [==============================] - 32s 360ms/step - loss: 3.0881 - accuracy: 0.2907\n",
      "Epoch 75/100\n",
      "89/89 [==============================] - 32s 357ms/step - loss: 3.0675 - accuracy: 0.2974\n",
      "Epoch 76/100\n",
      "89/89 [==============================] - 32s 360ms/step - loss: 3.0457 - accuracy: 0.3001\n",
      "Epoch 77/100\n",
      "89/89 [==============================] - 32s 360ms/step - loss: 3.0242 - accuracy: 0.3043\n",
      "Epoch 78/100\n",
      "89/89 [==============================] - 32s 363ms/step - loss: 2.9971 - accuracy: 0.3094\n",
      "Epoch 79/100\n",
      "89/89 [==============================] - 34s 377ms/step - loss: 2.9732 - accuracy: 0.3147\n",
      "Epoch 80/100\n",
      "89/89 [==============================] - 32s 359ms/step - loss: 2.9454 - accuracy: 0.3167\n",
      "Epoch 81/100\n",
      "89/89 [==============================] - 32s 359ms/step - loss: 2.9224 - accuracy: 0.3211\n",
      "Epoch 82/100\n",
      "89/89 [==============================] - 32s 361ms/step - loss: 2.8989 - accuracy: 0.3259\n",
      "Epoch 83/100\n",
      "89/89 [==============================] - 32s 362ms/step - loss: 2.8860 - accuracy: 0.3260\n",
      "Epoch 84/100\n",
      "89/89 [==============================] - 32s 360ms/step - loss: 2.8624 - accuracy: 0.3315\n",
      "Epoch 85/100\n",
      "89/89 [==============================] - 32s 361ms/step - loss: 2.8407 - accuracy: 0.3367\n",
      "Epoch 86/100\n",
      "89/89 [==============================] - 32s 363ms/step - loss: 2.8150 - accuracy: 0.3392\n",
      "Epoch 87/100\n",
      "89/89 [==============================] - 32s 362ms/step - loss: 2.7980 - accuracy: 0.3434\n",
      "Epoch 88/100\n",
      "89/89 [==============================] - 32s 364ms/step - loss: 2.7759 - accuracy: 0.3452\n",
      "Epoch 89/100\n",
      "89/89 [==============================] - 32s 360ms/step - loss: 2.7541 - accuracy: 0.3491\n",
      "Epoch 90/100\n",
      "89/89 [==============================] - 32s 358ms/step - loss: 2.7350 - accuracy: 0.3554\n",
      "Epoch 91/100\n",
      "89/89 [==============================] - 32s 359ms/step - loss: 2.7104 - accuracy: 0.3581\n",
      "Epoch 92/100\n",
      "89/89 [==============================] - 32s 360ms/step - loss: 2.6947 - accuracy: 0.3620\n",
      "Epoch 93/100\n",
      "89/89 [==============================] - 32s 356ms/step - loss: 2.6699 - accuracy: 0.3668\n",
      "Epoch 94/100\n",
      "89/89 [==============================] - 32s 360ms/step - loss: 2.6565 - accuracy: 0.3687\n",
      "Epoch 95/100\n",
      "89/89 [==============================] - 32s 364ms/step - loss: 2.6304 - accuracy: 0.3763\n",
      "Epoch 96/100\n",
      "89/89 [==============================] - 32s 361ms/step - loss: 2.6146 - accuracy: 0.3766\n",
      "Epoch 97/100\n",
      "89/89 [==============================] - 32s 360ms/step - loss: 2.5934 - accuracy: 0.3816\n",
      "Epoch 98/100\n",
      "89/89 [==============================] - 33s 366ms/step - loss: 2.5775 - accuracy: 0.3860\n",
      "Epoch 99/100\n",
      "89/89 [==============================] - 32s 359ms/step - loss: 2.5589 - accuracy: 0.3870\n",
      "Epoch 100/100\n",
      "89/89 [==============================] - 32s 358ms/step - loss: 2.5399 - accuracy: 0.3942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x247f54de670>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size = 256, epochs = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "707fbb2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'condition seemed serious enough to remind even his father that gregor despite his current sad and revolting form was a family member who could not be treated as an enemy on the contrary as a family there was a duty to swallow any revulsion for him and to be patient just'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text=lines[12343]\n",
    "seed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59ad7e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_seq(model, tokenizer, text_seq_length, seed_text, n_words):\n",
    " text = []\n",
    " for _ in range(n_words):\n",
    "     encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "     encoded = pad_sequences([encoded], maxlen = text_seq_length, truncating='pre')\n",
    "     y_predict = model.predict_classes(encoded)\n",
    "     predicted_word = ''\n",
    "     for word, index in tokenizer.word_index.items():\n",
    "        if index == y_predict:\n",
    "         predicted_word = word\n",
    "         break\n",
    "     seed_text = seed_text + ' ' + predicted_word\n",
    "     text.append(predicted_word)\n",
    " return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6efb7415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the family was not a commercial discourtesy and your back with the street towards the stairway and was not a lot of year to pay attention to him mixed than it was sufficiently cooked with the door where it was something to find up human beings to live for getting backwards to the door and was glad to be closed in the practise he could not seen outstretched to him to see it he was not to turn round he was weak of disturbing him on the evenings and the only youre used to learn the chief clerk was saying'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_seq(model, tokenizer, seq_length, seed_text, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfb6de5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
